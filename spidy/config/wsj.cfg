THREAD_COUNT = 1
OVERWRITE = False
RAISE_ERRORS = False
SAVE_PAGES = True
SAVE_WORDS = False
ZIP_FILES = False
OVERRIDE_SIZE = False

# Whether to restrict crawling to a single domain or not.
RESTRICT = True

# The domain within which to restrict crawling.
DOMAIN = 'wsj.com/'

RESPECT_ROBOTS = True
TODO_FILE = 'wsj_todo.txt'
DONE_FILE = 'wsj_done.txt'
WORD_FILE = 'wsj_words.txt'
SAVE_COUNT = 60
HEADER = HEADERS['spidy']
MAX_NEW_ERRORS = 100
MAX_KNOWN_ERRORS = 100
MAX_HTTP_ERRORS = 100
MAX_NEW_MIMES = 5
START = ['https://www.wsj.com/']
